{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torch.nn.functional as F\n",
    "from IPython import display\n",
    "from tqdm.notebook import tqdm\n",
    "import random\n",
    "import math, time, os\n",
    "from matplotlib import pyplot as plt\n",
    "import pickle as pkl\n",
    "\n",
    "from AutoEncoder import Autoencoder as AE\n",
    "from copy import deepcopy\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "device = torch.device('cuda:2' if torch.cuda.is_available() else 'cpu')\n",
    "prefix = './data_dump'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../Datasets/df_train_shuffled.csv (175127, 28)\n",
      "../Datasets/df_val.csv (52212, 28)\n",
      "../Datasets/df_val_test.csv (56802, 28)\n"
     ]
    }
   ],
   "source": [
    "# day checker\n",
    "file_paths = ['../Datasets/df_train_shuffled.csv', \n",
    "              '../Datasets/df_val.csv',\n",
    "              '../Datasets/df_val_test.csv']\n",
    "\n",
    "for file in file_paths:\n",
    "#     print(\"NEW FILE\")\n",
    "    df = pd.read_csv(file)\n",
    "    print(file, df.shape)\n",
    "#     print(np.unique(df['day_no'].value_counts().to_numpy()))\n",
    "\n",
    "    # cur_day = df['day_no'][0]\n",
    "    # cur_era = df['era'][0]\n",
    "    # for i in range(1, df.shape[0]):\n",
    "    #     if df['day_no'][i] == cur_day and df['era'][i] == cur_era:\n",
    "    #         pass\n",
    "    #     elif df['day_no'][i] == cur_day and df['era'][i] != cur_era:\n",
    "    #         print(\"1\")\n",
    "    #     elif df['day_no'][i] != cur_day and df['era'][i] == cur_era:\n",
    "    #         print(\"2\")\n",
    "    #     else:\n",
    "    #         pass\n",
    "    #     cur_era = df['era'][i]\n",
    "    #     cur_day = df['day_no'][i] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SinusodialDataset(Dataset):\n",
    "    def __init__(self, df, test=False):\n",
    "        \"\"\" creating label columns of eras and targets \"\"\"\n",
    "        self.NUM_FEATURES = 24\n",
    "        self.X = df.iloc[:, :self.NUM_FEATURES]\n",
    "        self.testMode = test\n",
    "        if self.testMode == False:\n",
    "            self.y = df['target_10_val']\n",
    "        self.X = self.create_categorical_one_hot(self.X)\n",
    "    \n",
    "    def create_categorical_one_hot(self, df):\n",
    "        categories = [0, 0.25, 0.5, 0.75, 1]\n",
    "        one_hot_encoded_columns = []\n",
    "        for col in df.columns:\n",
    "            for cat in categories:\n",
    "                new_col_name = f\"{col}_{cat}\"\n",
    "                one_hot_encoded_col = (df[col] == cat).astype(int)\n",
    "                one_hot_encoded_col.name = new_col_name\n",
    "                one_hot_encoded_columns.append(one_hot_encoded_col)\n",
    "\n",
    "        # Concatenate the one-hot encoded columns along axis 1\n",
    "        return pd.concat(one_hot_encoded_columns, axis=1)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        X = torch.tensor(self.X.iloc[idx].values, dtype=torch.float32)\n",
    "        if self.testMode == False:\n",
    "            y = torch.tensor(self.y.iloc[idx], dtype=torch.long)\n",
    "            return X, y\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_data_splits(train_df, val_df, test_df, batch_size=32):\n",
    "\n",
    "    def encode(v, class_values):\n",
    "        return class_values.index(v)\n",
    "\n",
    "    class_values = train_df['target_10_val'].unique().tolist()\n",
    "    train_df['target_10_val'] = train_df['target_10_val'].apply(lambda x: encode(x, class_values))\n",
    "    train_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    data_train = SinusodialDataset(train_df)\n",
    "    loader_train = DataLoader(data_train, batch_size=batch_size, shuffle=False)\n",
    "    loader_val = None\n",
    "    loader_test = None\n",
    "\n",
    "    if val_df is not None:\n",
    "        class_values = val_df['target_10_val'].unique().tolist()\n",
    "        val_df['target_10_val'] = val_df['target_10_val'].apply(lambda x: encode(x, class_values))\n",
    "        val_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        data_val = SinusodialDataset(val_df)\n",
    "        loader_val = DataLoader(data_val, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    if test_df is not None:\n",
    "        class_values = test_df['target_10_val'].unique().tolist()\n",
    "        test_df['target_10_val'] = test_df['target_10_val'].apply(lambda x: encode(x, class_values))\n",
    "        test_df.reset_index(drop=True, inplace=True)\n",
    "            \n",
    "        data_test = SinusodialDataset(test_df)\n",
    "        loader_test = DataLoader(data_test, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    print(\"Train-val-test lengths: \", len(data_train), len(data_val), len(data_test))\n",
    "\n",
    "    return loader_train, loader_val, loader_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestTimeAdapter(nn.Module):\n",
    "    \n",
    "    def __init__(self, ae_dims, cl_dims, lr=1e-3, weight_decay=1e-3):\n",
    "        super(TestTimeAdapter,self).__init__()\n",
    "        self.ae_dims=ae_dims\n",
    "        self.cl_dims = cl_dims\n",
    "\n",
    "        self.ae = AE(ae_dims)\n",
    "        self.classifier=nn.ModuleList()\n",
    "        \n",
    "        for i in range(len(cl_dims)-2):\n",
    "            self.classifier.append(nn.Linear(cl_dims[i],cl_dims[i+1]))\n",
    "            self.classifier.append(nn.ReLU())\n",
    "        self.classifier.append(nn.Linear(cl_dims[i+1],cl_dims[i+2]))\n",
    "        self.classifier.append(nn.LogSoftmax(dim=1))\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    def forward(self, x, classifier=False):\n",
    "        if classifier:\n",
    "            x = x.float()\n",
    "            x = self.ae.encode(x)\n",
    "            for l in self.classifier:\n",
    "                x = l(x)\n",
    "            return x\n",
    "        \n",
    "        x = x.float()\n",
    "        x = self.ae(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_pred, y_test, verbose=True):\n",
    "    m = y_test.shape[0]\n",
    "    predicted = torch.max(y_pred, 1)[1]\n",
    "    correct = (predicted == y_test).float().sum().item()\n",
    "    if verbose:\n",
    "        print(correct,m)\n",
    "    accuracy = correct/m\n",
    "    return accuracy, correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Test(net, loader_test, \\\n",
    "         device='cpu', Loss=nn.NLLLoss(reduction='sum'), \\\n",
    "         test_time_epochs = 3):\n",
    "    net.train()\n",
    "    total_samples = 0\n",
    "    correct_samples = 0\n",
    "    loss = 0.0\n",
    "    step=0\n",
    "    (X_prev, y_prev) = (None, None)\n",
    "    (X_prev_prev, y_prev_prev) = (None, None)\n",
    "    # test_mode is False\n",
    "    for (X, y) in loader_test:\n",
    "        # print(step, end=\" \")\n",
    "        X=X.to(device)\n",
    "        y=y.to(device)\n",
    "        total_samples += y.shape[0]\n",
    "   \n",
    "        for e in range(test_time_epochs): \n",
    "            x_reconst = net(X, classifier=False)\n",
    "            ae_loss = 0.0\n",
    "            for feat in range(0, X.shape[-1], 5):\n",
    "                ae_loss += Loss(nn.LogSoftmax(dim=1)(x_reconst[:, feat:feat+5]),X[:, feat:feat+5].argmax(dim=1))\n",
    "            net.optimizer.zero_grad()\n",
    "            ae_loss.backward()\n",
    "            net.optimizer.step()\n",
    "        \n",
    "            if X_prev_prev is not None:\n",
    "                y_pred = net(X_prev_prev, classifier=True)\n",
    "                cl_loss = Loss(y_pred, y_prev_prev)\n",
    "                net.optimizer.zero_grad()\n",
    "                cl_loss.backward()\n",
    "                net.optimizer.step()\n",
    "\n",
    "        y_pred = net(X, classifier=True)\n",
    "        cl_loss = Loss(y_pred, y)\n",
    "\n",
    "        loss += (ae_loss+cl_loss).item()\n",
    "        _, i_cor_sam = accuracy(y_pred, y,verbose=False)\n",
    "        correct_samples += i_cor_sam\n",
    "        step+=1\n",
    "\n",
    "        (X_prev_prev, y_prev_prev) = (X_prev, y_prev)\n",
    "        (X_prev, y_prev) = (X, y)\n",
    "\n",
    "    # print()\n",
    "    \n",
    "    acc = correct_samples / total_samples\n",
    "    loss /= total_samples\n",
    "    print('Test/Val loss:', loss, 'Test/Val acc:', acc)\n",
    "    return loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Train(Net, train_loader, epochs=20, Loss=nn.NLLLoss(reduction='sum'), \n",
    "          verbose=False, device='cpu',\n",
    "          val_ds=None, loader_test=None):\n",
    "    model_save_time = time.time()\n",
    "    losses = []\n",
    "    accs = []\n",
    "    val_losses=[]\n",
    "    val_accL=[]\n",
    "    Net.to(device)\n",
    "    for e in range(epochs):\n",
    "        Net.train()\n",
    "        step=0\n",
    "        tot_loss=0.0\n",
    "        start_time = time.time()\n",
    "        correct_samples = 0\n",
    "        total_samples = 0\n",
    "        # test_mode = False\n",
    "        for (X,y) in train_loader:\n",
    "            X=X.to(device)\n",
    "            y=y.to(device)\n",
    "            total_samples += y.shape[0]\n",
    "            x_reconst = Net(X, classifier=False) # B x nd x nc = B x 24 x 5\n",
    "            ae_loss = 0.0\n",
    "            for feat in range(0, X.shape[-1], 5):\n",
    "                # print((x_reconst[:, feat:feat+5]),X[:, feat:feat+5].argmax(dim=1))\n",
    "                # print((x_reconst[:, feat:feat+5]).shape,X[:, feat:feat+5].argmax(dim=1).shape)\n",
    "                ae_loss += Loss(nn.LogSoftmax(dim=1)(x_reconst[:, feat:feat+5]),X[:, feat:feat+5].argmax(dim=1))\n",
    "            Net.optimizer.zero_grad()\n",
    "            ae_loss.backward()\n",
    "            Net.optimizer.step()\n",
    "\n",
    "            y_pred = Net(X, classifier=True)\n",
    "            cl_loss = Loss(y_pred, y)\n",
    "            Net.optimizer.zero_grad()\n",
    "            cl_loss.backward()\n",
    "            Net.optimizer.step()\n",
    "\n",
    "            step+=1\n",
    "            tot_loss+=(ae_loss+cl_loss)\n",
    "            if verbose:\n",
    "                _, i_cor_sam = accuracy(y_pred, y,verbose=False)\n",
    "                correct_samples += i_cor_sam\n",
    "            \n",
    "        end_time = time.time()\n",
    "        t = end_time-start_time\n",
    "        l = tot_loss.item()/total_samples\n",
    "        losses += [l]\n",
    "        a = correct_samples/total_samples\n",
    "        accs += [a]\n",
    "\n",
    "        if verbose:\n",
    "            print('Epoch %2d Loss: %2.5e Accuracy: %2.5f Epoch Time: %2.5f' %(e,l,a,t))\n",
    "\n",
    "        val_loss, val_acc = Test(deepcopy(Net), val_ds, device = device)\n",
    "        val_losses.append(val_loss)\n",
    "        val_accL.append(val_acc)\n",
    "\n",
    "        # print(\"TESTING BUDDY:\")\n",
    "        # Test(deepcopy(Net), loader_test, device=device)\n",
    "\n",
    "        torch.save(Net.state_dict(), f'{prefix}/net_{str(model_save_time)}.pth')\n",
    "\n",
    "    return Net, losses, accs, val_losses, val_accL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_acc(losses, accs, val_losses, val_accs):\n",
    "\n",
    "    plt.plot(np.array(accs),color='red', label='Train accuracy')\n",
    "    plt.plot(np.array(val_accs),color='blue', label='Val accuracy')\n",
    "    plt.legend()\n",
    "    plt.savefig(f'{prefix}/acc_tta.png')\n",
    "    plt.clf()\n",
    "\n",
    "    plt.plot(np.array(losses),color='red', label='Train loss')\n",
    "    plt.plot(np.array(val_losses),color='blue', label='Val loss')\n",
    "    plt.legend()\n",
    "    plt.savefig(f'{prefix}/loss_TTA.png')\n",
    "    plt.clf()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train-val-test lengths:  175127 52212 56802\n",
      "Epoch  0 Loss: 1.71056e+01 Accuracy: 0.26179 Epoch Time: 24.21161\n",
      "Test/Val loss: 12.69470003958471 Test/Val acc: 0.28390025281544473\n",
      "Epoch  1 Loss: 1.14282e+01 Accuracy: 0.27719 Epoch Time: 21.28432\n",
      "Test/Val loss: 10.732439854022124 Test/Val acc: 0.2877882479123573\n",
      "Epoch  2 Loss: 9.74013e+00 Accuracy: 0.27921 Epoch Time: 19.09744\n",
      "Test/Val loss: 9.91403279603692 Test/Val acc: 0.28918639393242934\n",
      "Epoch  3 Loss: 9.01222e+00 Accuracy: 0.27961 Epoch Time: 19.88013\n",
      "Test/Val loss: 9.38526553533401 Test/Val acc: 0.28631349115145943\n",
      "Epoch  4 Loss: 8.50728e+00 Accuracy: 0.27980 Epoch Time: 22.40702\n",
      "Test/Val loss: 9.057438463395028 Test/Val acc: 0.289626905692178\n",
      "Epoch  5 Loss: 8.22294e+00 Accuracy: 0.28082 Epoch Time: 20.54733\n",
      "Test/Val loss: 8.854935711739593 Test/Val acc: 0.292748793380832\n",
      "Epoch  6 Loss: 8.07467e+00 Accuracy: 0.28035 Epoch Time: 19.96255\n",
      "Test/Val loss: 8.702982484102524 Test/Val acc: 0.2954301693097372\n",
      "Epoch  7 Loss: 7.94375e+00 Accuracy: 0.28065 Epoch Time: 19.27895\n",
      "Test/Val loss: 8.563475457807158 Test/Val acc: 0.2955642381061825\n",
      "Epoch  8 Loss: 7.78979e+00 Accuracy: 0.28128 Epoch Time: 20.04650\n",
      "Test/Val loss: 8.393598466068175 Test/Val acc: 0.29265302995479964\n",
      "Epoch  9 Loss: 7.65062e+00 Accuracy: 0.28126 Epoch Time: 19.15104\n",
      "Test/Val loss: 8.205726604246868 Test/Val acc: 0.2931318470849613\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at inline_container.cc:337] . unexpected pos 128 vs 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/tiger/lib/python3.9/site-packages/torch/serialization.py:441\u001b[0m, in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[1;32m    440\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _open_zipfile_writer(f) \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[0;32m--> 441\u001b[0m     \u001b[43m_save\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_protocol\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    442\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/tiger/lib/python3.9/site-packages/torch/serialization.py:655\u001b[0m, in \u001b[0;36m_save\u001b[0;34m(obj, zip_file, pickle_module, pickle_protocol)\u001b[0m\n\u001b[1;32m    654\u001b[0m data_value \u001b[38;5;241m=\u001b[39m data_buf\u001b[38;5;241m.\u001b[39mgetvalue()\n\u001b[0;32m--> 655\u001b[0m \u001b[43mzip_file\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_record\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdata.pkl\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata_value\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    657\u001b[0m \u001b[38;5;66;03m# Write each tensor to a file named tensor/the_tensor_key in the zip archive\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: [enforce fail at inline_container.cc:471] . PytorchStreamWriter failed writing file data.pkl: file write failed",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[48], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m loader_train, loader_val, loader_test \u001b[38;5;241m=\u001b[39m make_data_splits(train_df, val_df, test_df, batch_size\u001b[38;5;241m=\u001b[39mbatch_size)\n\u001b[1;32m     12\u001b[0m net \u001b[38;5;241m=\u001b[39m TestTimeAdapter(ae_dims\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m120\u001b[39m, \u001b[38;5;241m64\u001b[39m, \u001b[38;5;241m32\u001b[39m, \u001b[38;5;241m16\u001b[39m, \u001b[38;5;241m32\u001b[39m, \u001b[38;5;241m64\u001b[39m, \u001b[38;5;241m120\u001b[39m], \n\u001b[1;32m     13\u001b[0m                         cl_dims\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m16\u001b[39m, \u001b[38;5;241m16\u001b[39m, \u001b[38;5;241m5\u001b[39m], )\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 14\u001b[0m net, losses, accs, val_losses, val_accL \u001b[38;5;241m=\u001b[39m \u001b[43mTrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloader_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mval_ds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloader_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43mloader_test\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloader_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m plot_loss_acc(losses, accs, val_losses, val_accL)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m#Testing code\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[37], line 61\u001b[0m, in \u001b[0;36mTrain\u001b[0;34m(Net, train_loader, epochs, Loss, verbose, device, val_ds, loader_test)\u001b[0m\n\u001b[1;32m     56\u001b[0m     val_accL\u001b[38;5;241m.\u001b[39mappend(val_acc)\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;66;03m# print(\"TESTING BUDDY:\")\u001b[39;00m\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;66;03m# Test(deepcopy(Net), loader_test, device=device)\u001b[39;00m\n\u001b[0;32m---> 61\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mNet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mprefix\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/net_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel_save_time\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.pth\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Net, losses, accs, val_losses, val_accL\n",
      "File \u001b[0;32m~/anaconda3/envs/tiger/lib/python3.9/site-packages/torch/serialization.py:442\u001b[0m, in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[1;32m    440\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m _open_zipfile_writer(f) \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[1;32m    441\u001b[0m         _save(obj, opened_zipfile, pickle_module, pickle_protocol)\n\u001b[0;32m--> 442\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    443\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    444\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m _open_file_like(f, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n",
      "File \u001b[0;32m~/anaconda3/envs/tiger/lib/python3.9/site-packages/torch/serialization.py:291\u001b[0m, in \u001b[0;36m_open_zipfile_writer_file.__exit__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    290\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__exit__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 291\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfile_like\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_end_of_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: [enforce fail at inline_container.cc:337] . unexpected pos 128 vs 0"
     ]
    }
   ],
   "source": [
    "df_paths = ['../Datasets/df_train_shuffled.csv', \n",
    "              '../Datasets/df_val.csv',\n",
    "              '../Datasets/df_val_test.csv']\n",
    "\n",
    "batch_size = 140\n",
    "\n",
    "train_df = pd.read_csv(df_paths[0])\n",
    "val_df = pd.read_csv(df_paths[1])\n",
    "test_df = pd.read_csv(df_paths[2])\n",
    "\n",
    "loader_train, loader_val, loader_test = make_data_splits(train_df, val_df, test_df, batch_size=batch_size)\n",
    "net = TestTimeAdapter(ae_dims=[120, 64, 32, 16, 32, 64, 120], \n",
    "                        cl_dims=[16, 16, 5], ).to(device)\n",
    "net, losses, accs, val_losses, val_accL = Train(net, loader_train, \\\n",
    "                                epochs=20, verbose=True, device=device, \\\n",
    "                                    val_ds=loader_val,loader_test=loader_test)\n",
    "plot_loss_acc(losses, accs, val_losses, val_accL)\n",
    "#Testing code\n",
    "test_loss, test_acc = Test(deepcopy(net), loader_test, device=device)\n",
    "print(test_loss, test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TestTime(net, loader_test, \\\n",
    "         device='cpu', Loss=nn.NLLLoss(reduction='sum'), \\\n",
    "         test_time_epochs = 3):\n",
    "    net.train()\n",
    "    total_samples = 0\n",
    "    loss = 0.0\n",
    "    step=0\n",
    "    \n",
    "    # test_mode is True\n",
    "    for X in loader_test:\n",
    "        X=X.to(device)\n",
    "        \n",
    "        total_samples += X.shape[0]\n",
    "   \n",
    "        for e in range(test_time_epochs): \n",
    "            x_reconst = net(X, classifier=False)\n",
    "            ae_loss = 0.0\n",
    "            for feat in range(0, X.shape[-1], 5):\n",
    "                ae_loss += Loss(nn.LogSoftmax(dim=1)(x_reconst[:, feat:feat+5]),X[:, feat:feat+5].argmax(dim=1))\n",
    "            net.optimizer.zero_grad()\n",
    "            ae_loss.backward()\n",
    "            net.optimizer.step()\n",
    "\n",
    "        loss += (ae_loss).item()\n",
    "        step+=1\n",
    "    \n",
    "    loss /= total_samples\n",
    "    print('Test/Val loss:', loss)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on -  ../Datasets/live_data_02-Apr-2024/df_live_train_02-Apr-2024_129.csv\n",
      "Test/Val loss: 25.671604070907986 Test/Val acc: 0.2846153846153846\n",
      "Working on -  ../Datasets/live_data_02-Apr-2024/df_live_test_02-Apr-2024_129.csv\n",
      "Test/Val loss: 21.708186467488606\n",
      "[array([2])]\n",
      "[array([2]), array([4])]\n",
      "[array([2]), array([4]), array([2])]\n",
      "Working on -  ../Datasets/live_data_02-Apr-2024/df_live_train_02-Apr-2024_122.csv\n",
      "Test/Val loss: 24.17695481215066 Test/Val acc: 0.2601626016260163\n",
      "Working on -  ../Datasets/live_data_02-Apr-2024/df_live_test_02-Apr-2024_122.csv\n",
      "Test/Val loss: 13.774435997009277\n",
      "[array([0])]\n",
      "Working on -  ../Datasets/live_data_02-Apr-2024/df_live_train_02-Apr-2024_143.csv\n",
      "Test/Val loss: 22.293966385170265 Test/Val acc: 0.18518518518518517\n",
      "Working on -  ../Datasets/live_data_02-Apr-2024/df_live_test_02-Apr-2024_143.csv\n",
      "Test/Val loss: 22.326815923055012\n",
      "[array([0])]\n",
      "[array([0]), array([0])]\n",
      "[array([0]), array([0]), array([0])]\n",
      "Working on -  ../Datasets/live_data_02-Apr-2024/df_live_train_02-Apr-2024_86.csv\n",
      "Test/Val loss: 21.98040099527644 Test/Val acc: 0.20689655172413793\n",
      "Working on -  ../Datasets/live_data_02-Apr-2024/df_live_test_02-Apr-2024_86.csv\n",
      "Test/Val loss: 16.910612106323242\n",
      "[array([2])]\n",
      "Working on -  ../Datasets/live_data_02-Apr-2024/df_live_train_02-Apr-2024_107.csv\n",
      "Test/Val loss: 21.317371947677046 Test/Val acc: 0.4203703703703704\n",
      "Working on -  ../Datasets/live_data_02-Apr-2024/df_live_test_02-Apr-2024_107.csv\n",
      "Test/Val loss: 16.107471466064453\n",
      "[array([2])]\n",
      "[array([2]), array([2])]\n",
      "[array([2]), array([2]), array([2])]\n",
      "[array([2]), array([2]), array([2]), array([2])]\n",
      "[array([2]), array([2]), array([2]), array([2]), array([2])]\n",
      "Working on -  ../Datasets/live_data_02-Apr-2024/df_live_train_02-Apr-2024_137.csv\n",
      "Test/Val loss: 19.47397321894549 Test/Val acc: 0.34782608695652173\n",
      "Working on -  ../Datasets/live_data_02-Apr-2024/df_live_test_02-Apr-2024_137.csv\n",
      "Test/Val loss: 13.468772888183594\n",
      "[array([1])]\n",
      "Working on -  ../Datasets/live_data_02-Apr-2024/df_live_train_02-Apr-2024_96.csv\n",
      "Test/Val loss: 23.82192730657833 Test/Val acc: 0.31958762886597936\n",
      "Working on -  ../Datasets/live_data_02-Apr-2024/df_live_test_02-Apr-2024_96.csv\n",
      "Test/Val loss: 19.53643862406413\n",
      "[array([4])]\n",
      "[array([4]), array([4])]\n",
      "[array([4]), array([4]), array([2])]\n",
      "Working on -  ../Datasets/live_data_02-Apr-2024/df_live_train_02-Apr-2024_142.csv\n",
      "Test/Val loss: 18.19482427877146 Test/Val acc: 0.36363636363636365\n",
      "Working on -  ../Datasets/live_data_02-Apr-2024/df_live_test_02-Apr-2024_142.csv\n",
      "Test/Val loss: 13.545758247375488\n",
      "[array([1])]\n",
      "Working on -  ../Datasets/live_data_02-Apr-2024/df_live_train_02-Apr-2024_113.csv\n",
      "Test/Val loss: 22.047334704482765 Test/Val acc: 0.2850877192982456\n",
      "Working on -  ../Datasets/live_data_02-Apr-2024/df_live_test_02-Apr-2024_113.csv\n",
      "Test/Val loss: 13.963664531707764\n",
      "[array([0])]\n",
      "[array([0]), array([4])]\n",
      "Working on -  ../Datasets/live_data_02-Apr-2024/df_live_train_02-Apr-2024_128.csv\n",
      "Test/Val loss: 21.105165060715155 Test/Val acc: 0.3603238866396761\n",
      "Working on -  ../Datasets/live_data_02-Apr-2024/df_live_test_02-Apr-2024_128.csv\n",
      "Test/Val loss: 17.089346885681152\n",
      "[array([2])]\n",
      "[array([2]), array([0])]\n",
      "[array([2]), array([0]), array([4])]\n",
      "[array([2]), array([0]), array([4]), array([2])]\n",
      "Working on -  ../Datasets/live_data_02-Apr-2024/df_live_train_02-Apr-2024_140.csv\n",
      "Test/Val loss: 17.91624828095132 Test/Val acc: 0.3333333333333333\n",
      "Working on -  ../Datasets/live_data_02-Apr-2024/df_live_test_02-Apr-2024_140.csv\n",
      "Test/Val loss: 10.166975975036621\n",
      "[array([1])]\n",
      "Working on -  ../Datasets/live_data_02-Apr-2024/df_live_train_02-Apr-2024_99.csv\n",
      "Test/Val loss: 21.791838665008544 Test/Val acc: 0.11\n",
      "Working on -  ../Datasets/live_data_02-Apr-2024/df_live_test_02-Apr-2024_99.csv\n",
      "Test/Val loss: 12.207026481628418\n",
      "[array([1])]\n",
      "Working on -  ../Datasets/live_data_02-Apr-2024/df_live_train_02-Apr-2024_110.csv\n",
      "Test/Val loss: 20.56782003243764 Test/Val acc: 0.3108108108108108\n",
      "Working on -  ../Datasets/live_data_02-Apr-2024/df_live_test_02-Apr-2024_110.csv\n",
      "Test/Val loss: 17.938154935836792\n",
      "[array([4])]\n",
      "[array([4]), array([4])]\n",
      "[array([4]), array([4]), array([4])]\n",
      "[array([4]), array([4]), array([4]), array([4])]\n",
      "Working on -  ../Datasets/live_data_02-Apr-2024/df_live_train_02-Apr-2024_120.csv\n",
      "Test/Val loss: 20.355720747272503 Test/Val acc: 0.24517906336088155\n",
      "Working on -  ../Datasets/live_data_02-Apr-2024/df_live_test_02-Apr-2024_120.csv\n",
      "Test/Val loss: 15.616046269734701\n",
      "[array([2])]\n",
      "[array([2]), array([4])]\n",
      "[array([2]), array([4]), array([2])]\n",
      "Working on -  ../Datasets/live_data_02-Apr-2024/df_live_train_02-Apr-2024_117.csv\n",
      "Test/Val loss: 20.8552479479048 Test/Val acc: 0.3\n",
      "Working on -  ../Datasets/live_data_02-Apr-2024/df_live_test_02-Apr-2024_117.csv\n",
      "Test/Val loss: 16.369157552719116\n",
      "[array([2])]\n",
      "[array([2]), array([0])]\n",
      "[array([2]), array([0]), array([0])]\n",
      "[array([2]), array([0]), array([0]), array([2])]\n",
      "Working on -  ../Datasets/live_data_02-Apr-2024/df_live_train_02-Apr-2024_134.csv\n",
      "Test/Val loss: 19.21204153343483 Test/Val acc: 0.1962962962962963\n",
      "Working on -  ../Datasets/live_data_02-Apr-2024/df_live_test_02-Apr-2024_134.csv\n",
      "Test/Val loss: 12.029260158538818\n",
      "[array([4])]\n",
      "[array([4]), array([2])]\n",
      "Working on -  ../Datasets/live_data_02-Apr-2024/df_live_train_02-Apr-2024_108.csv\n",
      "Test/Val loss: 18.27630315133191 Test/Val acc: 0.3577981651376147\n",
      "Working on -  ../Datasets/live_data_02-Apr-2024/df_live_test_02-Apr-2024_108.csv\n",
      "Test/Val loss: 16.194220066070557\n",
      "[array([4])]\n",
      "[array([4]), array([4])]\n",
      "Working on -  ../Datasets/live_data_02-Apr-2024/df_live_train_02-Apr-2024_144.csv\n",
      "Test/Val loss: 21.15540109126213 Test/Val acc: 0.2837370242214533\n",
      "Working on -  ../Datasets/live_data_02-Apr-2024/df_live_test_02-Apr-2024_144.csv\n",
      "Test/Val loss: 17.555641174316406\n",
      "[array([4])]\n",
      "[array([4]), array([4])]\n",
      "Working on -  ../Datasets/live_data_02-Apr-2024/df_live_train_02-Apr-2024_130.csv\n",
      "Test/Val loss: 18.943173764012847 Test/Val acc: 0.37659033078880405\n",
      "Working on -  ../Datasets/live_data_02-Apr-2024/df_live_test_02-Apr-2024_130.csv\n",
      "Test/Val loss: 16.946598688761394\n",
      "[array([2])]\n",
      "[array([2]), array([2])]\n",
      "[array([2]), array([2]), array([4])]\n",
      "Working on -  ../Datasets/live_data_02-Apr-2024/df_live_train_02-Apr-2024_141.csv\n",
      "Test/Val loss: 19.008685162369634 Test/Val acc: 0.3873239436619718\n",
      "Working on -  ../Datasets/live_data_02-Apr-2024/df_live_test_02-Apr-2024_141.csv\n",
      "Test/Val loss: 16.878603219985962\n",
      "[array([2])]\n",
      "[array([2]), array([2])]\n",
      "Working on -  ../Datasets/live_data_02-Apr-2024/df_live_train_02-Apr-2024_133.csv\n",
      "Test/Val loss: 18.674415952234124 Test/Val acc: 0.31716417910447764\n",
      "Working on -  ../Datasets/live_data_02-Apr-2024/df_live_test_02-Apr-2024_133.csv\n",
      "Test/Val loss: 14.12938940525055\n",
      "[array([2])]\n",
      "[array([2]), array([2])]\n",
      "[array([2]), array([2]), array([0])]\n",
      "[array([2]), array([2]), array([0]), array([4])]\n",
      "Working on -  ../Datasets/live_data_02-Apr-2024/df_live_train_02-Apr-2024_104.csv\n",
      "Test/Val loss: 18.545166769481842 Test/Val acc: 0.4380952380952381\n",
      "Working on -  ../Datasets/live_data_02-Apr-2024/df_live_test_02-Apr-2024_104.csv\n",
      "Test/Val loss: 9.346231460571289\n",
      "[array([2])]\n",
      "Working on -  ../Datasets/live_data_02-Apr-2024/df_live_train_02-Apr-2024_94.csv\n",
      "Test/Val loss: 24.588330058047646 Test/Val acc: 0.12631578947368421\n",
      "Working on -  ../Datasets/live_data_02-Apr-2024/df_live_test_02-Apr-2024_94.csv\n",
      "Test/Val loss: 13.42013931274414\n",
      "[array([0])]\n",
      "Working on -  ../Datasets/live_data_02-Apr-2024/df_live_train_02-Apr-2024_92.csv\n",
      "Test/Val loss: 20.360405429717034 Test/Val acc: 0.17204301075268819\n",
      "Working on -  ../Datasets/live_data_02-Apr-2024/df_live_test_02-Apr-2024_92.csv\n",
      "Test/Val loss: 18.925500869750977\n",
      "[array([4])]\n",
      "Working on -  ../Datasets/live_data_02-Apr-2024/df_live_train_02-Apr-2024_127.csv\n",
      "Test/Val loss: 18.430291198257706 Test/Val acc: 0.17094017094017094\n",
      "Working on -  ../Datasets/live_data_02-Apr-2024/df_live_test_02-Apr-2024_127.csv\n",
      "Test/Val loss: 14.787668704986572\n",
      "[array([4])]\n",
      "[array([4]), array([4])]\n",
      "Working on -  ../Datasets/live_data_02-Apr-2024/df_live_train_02-Apr-2024_105.csv\n",
      "Test/Val loss: 17.34918496743688 Test/Val acc: 0.36792452830188677\n",
      "Working on -  ../Datasets/live_data_02-Apr-2024/df_live_test_02-Apr-2024_105.csv\n",
      "Test/Val loss: 14.481195449829102\n",
      "[array([4])]\n",
      "Working on -  ../Datasets/live_data_02-Apr-2024/df_live_train_02-Apr-2024_123.csv\n",
      "Test/Val loss: 17.980758109400348 Test/Val acc: 0.4838709677419355\n",
      "Working on -  ../Datasets/live_data_02-Apr-2024/df_live_test_02-Apr-2024_123.csv\n",
      "Test/Val loss: 20.286794662475586\n",
      "[array([1])]\n",
      "Working on -  ../Datasets/live_data_02-Apr-2024/df_live_train_02-Apr-2024_131.csv\n",
      "Test/Val loss: 17.208007442228723 Test/Val acc: 0.30303030303030304\n",
      "Working on -  ../Datasets/live_data_02-Apr-2024/df_live_test_02-Apr-2024_131.csv\n",
      "Test/Val loss: 12.682973861694336\n",
      "[array([2])]\n",
      "[array([2]), array([2])]\n",
      "Working on -  ../Datasets/live_data_02-Apr-2024/df_live_train_02-Apr-2024_125.csv\n",
      "Test/Val loss: 15.552673445807564 Test/Val acc: 0.3253968253968254\n",
      "Working on -  ../Datasets/live_data_02-Apr-2024/df_live_test_02-Apr-2024_125.csv\n",
      "Test/Val loss: 15.738494396209717\n",
      "[array([0])]\n",
      "[array([0]), array([1])]\n",
      "Working on -  ../Datasets/live_data_02-Apr-2024/df_live_train_02-Apr-2024_132.csv\n",
      "Test/Val loss: 18.599724008803978 Test/Val acc: 0.24511278195488723\n",
      "Working on -  ../Datasets/live_data_02-Apr-2024/df_live_test_02-Apr-2024_132.csv\n",
      "Test/Val loss: 14.206049346923828\n",
      "[array([0])]\n",
      "[array([0]), array([0])]\n",
      "[array([0]), array([0]), array([1])]\n",
      "[array([0]), array([0]), array([1]), array([4])]\n",
      "[array([0]), array([0]), array([1]), array([4]), array([1])]\n",
      "Working on -  ../Datasets/live_data_02-Apr-2024/df_live_train_02-Apr-2024_93.csv\n",
      "Test/Val loss: 21.916057475069735 Test/Val acc: 0.11702127659574468\n",
      "Working on -  ../Datasets/live_data_02-Apr-2024/df_live_test_02-Apr-2024_93.csv\n",
      "Test/Val loss: 13.997992515563965\n",
      "[array([0])]\n",
      "Working on -  ../Datasets/live_data_02-Apr-2024/df_live_train_02-Apr-2024_102.csv\n",
      "Test/Val loss: 19.26304565355616 Test/Val acc: 0.27184466019417475\n",
      "Working on -  ../Datasets/live_data_02-Apr-2024/df_live_test_02-Apr-2024_102.csv\n",
      "Test/Val loss: 8.284529685974121\n",
      "[array([4])]\n",
      "Working on -  ../Datasets/live_data_02-Apr-2024/df_live_train_02-Apr-2024_121.csv\n",
      "Test/Val loss: 19.061438862058417 Test/Val acc: 0.1868583162217659\n",
      "Working on -  ../Datasets/live_data_02-Apr-2024/df_live_test_02-Apr-2024_121.csv\n",
      "Test/Val loss: 12.297536373138428\n",
      "[array([4])]\n",
      "[array([4]), array([4])]\n",
      "[array([4]), array([4]), array([4])]\n",
      "[array([4]), array([4]), array([4]), array([0])]\n",
      "Working on -  ../Datasets/live_data_02-Apr-2024/df_live_train_02-Apr-2024_136.csv\n",
      "Test/Val loss: 17.00124829006891 Test/Val acc: 0.43795620437956206\n",
      "Working on -  ../Datasets/live_data_02-Apr-2024/df_live_test_02-Apr-2024_136.csv\n",
      "Test/Val loss: 12.40652084350586\n",
      "[array([1])]\n",
      "Working on -  ../Datasets/live_data_02-Apr-2024/df_live_train_02-Apr-2024_124.csv\n",
      "Test/Val loss: 15.091792343139648 Test/Val acc: 0.416\n",
      "Working on -  ../Datasets/live_data_02-Apr-2024/df_live_test_02-Apr-2024_124.csv\n",
      "Test/Val loss: 14.069048404693604\n",
      "[array([0])]\n",
      "[array([0]), array([1])]\n",
      "Working on -  ../Datasets/live_data_02-Apr-2024/df_live_train_02-Apr-2024_126.csv\n",
      "Test/Val loss: 14.531594464159387 Test/Val acc: 0.18110236220472442\n",
      "Working on -  ../Datasets/live_data_02-Apr-2024/df_live_test_02-Apr-2024_126.csv\n",
      "Test/Val loss: 13.511398315429688\n",
      "[array([1])]\n",
      "Working on -  ../Datasets/live_data_02-Apr-2024/df_live_train_02-Apr-2024_135.csv\n",
      "Test/Val loss: 17.584620966630823 Test/Val acc: 0.2647058823529412\n",
      "Working on -  ../Datasets/live_data_02-Apr-2024/df_live_test_02-Apr-2024_135.csv\n",
      "Test/Val loss: 14.936090469360352\n",
      "[array([1])]\n",
      "Working on -  ../Datasets/live_data_02-Apr-2024/df_live_train_02-Apr-2024_138.csv\n",
      "Test/Val loss: 14.544889076150579 Test/Val acc: 0.4244604316546763\n",
      "Working on -  ../Datasets/live_data_02-Apr-2024/df_live_test_02-Apr-2024_138.csv\n",
      "Test/Val loss: 6.490586280822754\n",
      "[array([1])]\n",
      "Working on -  ../Datasets/live_data_02-Apr-2024/df_live_train_02-Apr-2024_106.csv\n",
      "Test/Val loss: 19.734497128245987 Test/Val acc: 0.34813084112149534\n",
      "Working on -  ../Datasets/live_data_02-Apr-2024/df_live_test_02-Apr-2024_106.csv\n",
      "Test/Val loss: 20.234434604644775\n",
      "[array([2])]\n",
      "[array([2]), array([0])]\n",
      "[array([2]), array([0]), array([2])]\n",
      "[array([2]), array([0]), array([2]), array([2])]\n",
      "Working on -  ../Datasets/live_data_02-Apr-2024/df_live_train_02-Apr-2024_97.csv\n",
      "Test/Val loss: 21.391244377163673 Test/Val acc: 0.16546762589928057\n",
      "Working on -  ../Datasets/live_data_02-Apr-2024/df_live_test_02-Apr-2024_97.csv\n",
      "Test/Val loss: 18.955191612243652\n",
      "[array([2])]\n",
      "[array([2]), array([3])]\n",
      "[array([2]), array([3]), array([2])]\n",
      "Working on -  ../Datasets/live_data_02-Apr-2024/df_live_train_02-Apr-2024_95.csv\n",
      "Test/Val loss: 19.491254687309265 Test/Val acc: 0.10416666666666667\n",
      "Working on -  ../Datasets/live_data_02-Apr-2024/df_live_test_02-Apr-2024_95.csv\n",
      "Test/Val loss: 21.18693733215332\n",
      "[array([4])]\n",
      "Working on -  ../Datasets/live_data_02-Apr-2024/df_live_train_02-Apr-2024_84.csv\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 37\u001b[0m\n\u001b[1;32m     35\u001b[0m data \u001b[38;5;241m=\u001b[39m SinusodialDataset(df, test\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     36\u001b[0m loader_adapt \u001b[38;5;241m=\u001b[39m DataLoader(data, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m---> 37\u001b[0m adapt_loss, adapt_acc \u001b[38;5;241m=\u001b[39m \u001b[43mTest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloader_adapt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_time_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWorking on - \u001b[39m\u001b[38;5;124m'\u001b[39m, round_dict[\u001b[38;5;28mround\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     40\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(round_dict[\u001b[38;5;28mround\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "Cell \u001b[0;32mIn[36], line 22\u001b[0m, in \u001b[0;36mTest\u001b[0;34m(net, loader_test, device, Loss, test_time_epochs)\u001b[0m\n\u001b[1;32m     20\u001b[0m ae_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m feat \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], \u001b[38;5;241m5\u001b[39m):\n\u001b[0;32m---> 22\u001b[0m     ae_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mLoss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLogSoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_reconst\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeat\u001b[49m\u001b[43m:\u001b[49m\u001b[43mfeat\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mX\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeat\u001b[49m\u001b[43m:\u001b[49m\u001b[43mfeat\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m net\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     24\u001b[0m ae_loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/anaconda3/envs/tiger/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/tiger/lib/python3.9/site-packages/torch/nn/modules/loss.py:216\u001b[0m, in \u001b[0;36mNLLLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnll_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tiger/lib/python3.9/site-packages/torch/nn/functional.py:2704\u001b[0m, in \u001b[0;36mnll_loss\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   2702\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2703\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 2704\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnll_loss_nd\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def encode(v, class_values):\n",
    "        return class_values.index(v)\n",
    "\n",
    "\n",
    "def list_csv_files(directory):\n",
    "    csv_files = [file for file in os.listdir(directory) if file.endswith('.csv')]\n",
    "    return csv_files\n",
    "\n",
    "for date in [2,3,4,5,8]:\n",
    "    file_path = f'../Datasets/live_data_0{date}-Apr-2024'\n",
    "    csv_files = list_csv_files(file_path)\n",
    "\n",
    "    round_dict = {}\n",
    "\n",
    "    for i in range(len(csv_files)):\n",
    "        round = int(csv_files[i].split(\".\")[0].split(\"_\")[-1])\n",
    "        if round not in round_dict.keys():\n",
    "            round_dict[round] = {'train': '', 'test': ''}\n",
    "        if 'train' in csv_files[i]:\n",
    "            round_dict[round]['train'] = file_path + '/' + csv_files[i]\n",
    "        else:\n",
    "            round_dict[round]['test'] = file_path + '/' + csv_files[i]\n",
    "            \n",
    "\n",
    "    results = pd.DataFrame()\n",
    "\n",
    "    for round in round_dict.keys():\n",
    "        print('Working on - ', round_dict[round]['train'])\n",
    "        df = pd.read_csv(round_dict[round]['train'])\n",
    "        class_values = df['target_10_val'].unique().tolist()\n",
    "        df['target_10_val'] = df['target_10_val'].apply(lambda x: encode(x, class_values))\n",
    "        df.reset_index(drop=True, inplace=True)\n",
    "        data = SinusodialDataset(df, test=False)\n",
    "        loader_adapt = DataLoader(data, batch_size=1, shuffle=False)\n",
    "        adapt_loss, adapt_acc = Test(net, loader_adapt, device=device, test_time_epochs=2)\n",
    "        \n",
    "        print('Working on - ', round_dict[round]['test'])\n",
    "        df = pd.read_csv(round_dict[round]['test'])\n",
    "\n",
    "        ids = df['id']\n",
    "        df = df.drop(columns=['id'])\n",
    "        row_num = df['row_num']\n",
    "        \n",
    "        data = SinusodialDataset(df, test=True)\n",
    "        loader_adapt = DataLoader(data, batch_size=1, shuffle=False)\n",
    "        adapt_loss = TestTime(net, loader_adapt, device=device)\n",
    "\n",
    "        predictions = []\n",
    "\n",
    "        total_samples = 0\n",
    "        for X in loader_adapt:\n",
    "            X=X.to(device)\n",
    "            total_samples += X.shape[0]\n",
    "            y_pred = net(X, classifier=True)\n",
    "            predictions.append(y_pred.argmax(dim=1).detach().cpu().numpy())\n",
    "            print(predictions)\n",
    "\n",
    "        temp_df = pd.DataFrame()\n",
    "        temp_df['id'] = ids\n",
    "        temp_df['predictions'] = predictions\n",
    "        temp_df['row_num'] = row_num\n",
    "        temp_df['round_no'] = [f'{round}']*ids.shape[0]\n",
    "        results = pd.concat([results, temp_df])\n",
    "    \n",
    "    \n",
    "    results.to_csv(f'predictions_0{date}-04-2024.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = Test(net, loader_val, device=device)\n",
    "print(test_loss, test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, dims, task='classification', lr=1e-2, weight_decay=0):\n",
    "        super(MLP,self).__init__()\n",
    "        self.dims=dims\n",
    "        self.task=task\n",
    "        self.layers=nn.ModuleList()\n",
    "        for i in range(len(self.dims)-2):\n",
    "            self.layers.append(nn.Linear(dims[i],dims[i+1]))\n",
    "            self.layers.append(nn.ReLU())\n",
    "        self.layers.append(nn.Linear(dims[i+1],dims[i+2]))\n",
    "        self.layers.append(nn.LogSoftmax(dim=1))\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = x.float()\n",
    "        for l in self.layers:\n",
    "            x = l(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "def accuracy(Net, X_test, y_test, verbose=True):\n",
    "    Net.eval()\n",
    "    m = X_test.shape[0]\n",
    "    y_pred = Net(X_test)\n",
    "    predicted = torch.max(y_pred, 1)[1]\n",
    "    correct = (predicted == y_test).float().sum().item()\n",
    "    if verbose:\n",
    "        print(correct,m)\n",
    "    accuracy = correct/m\n",
    "    Net.train()\n",
    "    return accuracy, correct\n",
    "\n",
    "\n",
    "def Test(net, loader_test, \\\n",
    "         device='cpu', Loss=nn.NLLLoss(reduction='sum')):\n",
    "    net.eval()\n",
    "    total_samples = 0\n",
    "    correct_samples = 0\n",
    "    loss = 0.0\n",
    "    for (X, y) in loader_test:\n",
    "        X=X.to(device)\n",
    "        y=y.to(device)\n",
    "        total_samples += y.shape[0]\n",
    "        _, i_cor_sam = accuracy(net,X,y,verbose=False)\n",
    "        correct_samples += i_cor_sam\n",
    "        loss += Loss(net(X), y).cpu().detach().item()\n",
    "    acc = correct_samples / total_samples\n",
    "    loss /= total_samples\n",
    "    return loss, acc\n",
    "\n",
    "def Train(Net, data, mode, noise_level, epochs=20, lr=5e-2, Loss=nn.NLLLoss(reduction='sum'), verbose=False, device='cpu',\n",
    "          val_ds=None, plot_accs=False, plot_losses=False):\n",
    "    model_save_time = time.time()\n",
    "    losses = []\n",
    "    accs = []\n",
    "    val_losses=[]\n",
    "    val_accL=[]\n",
    "    Net.to(device)\n",
    "    for e in range(epochs):\n",
    "        Net.train()\n",
    "        step=0\n",
    "        tot_loss=0.0\n",
    "        start_time = time.time()\n",
    "        correct_samples = 0\n",
    "        total_samples = 0\n",
    "        for (X,y) in data:\n",
    "            X=X.to(device)\n",
    "            y=y.to(device)\n",
    "            total_samples += y.shape[0]\n",
    "            y_pred = Net(X)\n",
    "            loss = Loss(y_pred,y)\n",
    "            Net.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            Net.optimizer.step()\n",
    "            step+=1\n",
    "            tot_loss+=loss\n",
    "            if verbose:\n",
    "                _, i_cor_sam = accuracy(Net,X,y,verbose=False)\n",
    "                correct_samples += i_cor_sam\n",
    "        end_time = time.time()\n",
    "        t = end_time-start_time\n",
    "        l = tot_loss.item()/total_samples\n",
    "        losses += [l]\n",
    "        a = correct_samples/total_samples\n",
    "        accs += [a]\n",
    "\n",
    "        if verbose:\n",
    "            print('Epoch %2d Loss: %2.5e Accuracy: %2.5f Epoch Time: %2.5f' %(e,l,a,t))\n",
    "\n",
    "        val_loss, val_acc = Test(Net, val_ds, mode, noise_level, device)\n",
    "        val_losses.append(val_loss)\n",
    "        val_accL.append(val_acc)\n",
    "\n",
    "        # Net.eval()\n",
    "        # if plot_accs and val_ds is not None:\n",
    "        #     val_total_samples = 0\n",
    "        #     val_correct_samples = 0\n",
    "        #     for (X, y) in val_ds:\n",
    "        #       X=X.to(device)\n",
    "        #       y=y.to(device)\n",
    "        #       val_total_samples += y.shape[0]\n",
    "        #       _, i_val_cor_sam = accuracy(Net,X,y,verbose=False)\n",
    "        #       val_correct_samples += i_val_cor_sam\n",
    "        #     val_accL+=[val_correct_samples / val_total_samples]\n",
    "        #     plt.plot(np.array(val_accL),color='red')\n",
    "        #     plt.plot(np.array(accs),color='blue')\n",
    "        #     plt.show()\n",
    "\n",
    "        # if plot_losses and val_ds is not None:\n",
    "        #     val_loss_ = 0.0\n",
    "        #     val_total_samples = 0\n",
    "        #     for (X, y) in val_ds:\n",
    "        #       X=X.to(device)\n",
    "        #       y=y.to(device)\n",
    "        #       val_total_samples += y.shape[0]\n",
    "        #       val_loss_ += Loss(Net(X), y).cpu().detach().item()\n",
    "        #     val_losses+=[val_loss_/val_total_samples]\n",
    "        #     plt.plot(val_losses,color='red')\n",
    "        #     plt.plot(losses,color='blue')\n",
    "        #     plt.show()\n",
    "\n",
    "        torch.save(Net.state_dict(), f'{prefix}/net_{noise_level}_{mode}_{str(model_save_time)}.pth')\n",
    "\n",
    "    return Net, losses, accs, val_losses, val_accL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_paths = ['../Datasets/df_train_shuffled.csv', \n",
    "              '../Datasets/df_val.csv',\n",
    "              '../Datasets/df_val_test.csv']\n",
    "\n",
    "batch_size = 140\n",
    "\n",
    "\n",
    "train_df = pd.read_csv(df_paths[0])\n",
    "val_df = pd.read_csv(df_paths[1])\n",
    "test_df = pd.read_csv(df_paths[2])\n",
    "\n",
    "loader_train, loader_val, loader_test = make_data_splits(train_df, val_df, test_df, batch_size=batch_size)\n",
    "net = MLP([24, 32, 5], lr=1e-3).to(device)\n",
    "net, losses, accs, val_losses, val_accL = Train(net, loader_train, '', '',\\\n",
    "                                epochs=20, verbose=True, device=device, \\\n",
    "                                    val_ds=loader_val)\n",
    "plot_loss_acc(losses, accs, val_losses, val_accL)\n",
    "#Testing code\n",
    "test_loss, test_acc = Test(deepcopy(net), loader_test, device=device)\n",
    "print(test_loss, test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Expert(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(Expert, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "class Gate(nn.Module):\n",
    "    def __init__(self, input_dim, num_experts):\n",
    "        super(Gate, self).__init__()\n",
    "        self.fc = nn.Linear(input_dim, num_experts)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.softmax(self.fc(x), dim=1)\n",
    "\n",
    "class MixtureOfExperts(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_experts):\n",
    "        super(MixtureOfExperts, self).__init__()\n",
    "        self.experts = nn.ModuleList([Expert(input_dim, hidden_dim, output_dim) for _ in range(num_experts)])\n",
    "        self.gate = Gate(input_dim, num_experts)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=0.001)\n",
    "\n",
    "    def forward(self, x):\n",
    "        gate_output = self.gate(x)\n",
    "        expert_outputs = [expert(x) for expert in self.experts]\n",
    "        expert_outputs = torch.stack(expert_outputs, dim=1)  # (batch_size, num_experts, output_dim)\n",
    "        prediction = torch.sum(gate_output.unsqueeze(2) * expert_outputs, dim=1)  # (batch_size, output_dim)\n",
    "        output = self.softmax(prediction)\n",
    "        return output\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    df_paths = ['../Datasets/df_train_shuffled.csv', \n",
    "              '../Datasets/df_val.csv',\n",
    "              '../Datasets/df_val_test.csv']\n",
    "\n",
    "    batch_size = 140\n",
    "\n",
    "\n",
    "    train_df = pd.read_csv(df_paths[0])\n",
    "    val_df = pd.read_csv(df_paths[1])\n",
    "    test_df = pd.read_csv(df_paths[2])\n",
    "\n",
    "    input_dim = 24\n",
    "    hidden_dim = 8\n",
    "    output_dim = 5\n",
    "    num_experts = 5\n",
    "\n",
    "    loader_train, loader_val, loader_test = make_data_splits(train_df, val_df, test_df, batch_size=batch_size)\n",
    "    net = MixtureOfExperts(input_dim, hidden_dim, output_dim, num_experts).to(device)\n",
    "    net, losses, accs, val_losses, val_accL = Train(net, loader_train, '', '',\\\n",
    "                                    epochs=20, verbose=True, device=device, \\\n",
    "                                        val_ds=loader_val)\n",
    "    plot_loss_acc(losses, accs, val_losses, val_accL)\n",
    "    #Testing code\n",
    "    test_loss, test_acc = Test(deepcopy(net), loader_test, device=device)\n",
    "    print(test_loss, test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tiger",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
